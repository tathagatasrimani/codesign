# LLaMA 3 8B full inference: prefill (prompt_len=64) + decode (64 tokens).
# Uses reduced dimensions (dim=256, ffn_dim=1024, n_heads=4).
# Prefill processes the prompt in one shot; decode generates tokens
# autoregressively, each decode step attending to the KV cache.
#
# The DFG is extracted automatically by tracing the LlamaInference
# PyTorch module's forward pass. Sub-module calls are recorded and
# pattern-matched to block_types via module_attr_map.
llama3_inference_system:
  name: "llama3_inference"
  description: "LLaMA 3 8B inference: prefill (64 tokens) + decode (64 tokens)"
  shared_tech: false

  global_memory:
    bandwidth_gbps: 100.0
    latency_ns: 100.0
    energy_per_bit_pj: 10.0
    capacity_bytes: 16.0e9

  # Benchmark module defines the inference workflow (prefill + decode loops).
  # system_codesign traces forward() to build the DFG automatically.
  benchmark:
    class: LlamaInference
    module_path: src/benchmarks/vitis/LlamaInference
    init_args:
      dim: 256
      n_heads: 4
      ffn_dim: 1024
      n_layers: 32
      n_decode: 64
      cache_len: 64
    input_shapes:
      - [1, 64, 256]           # prompt tensor (batch, prompt_len, dim)
    module_attr_map:
      prefill_attention: prefill_attention
      prefill_feedforward: prefill_feedforward
      decode_attention: decode_attention
      decode_feedforward: decode_feedforward

  block_types:
    # --- Prefill phase blocks (process full prompt) ---
    prefill_attention:
      codesign_config: vitis_test_sweep_basic_llama_prefill_attention
      shared_weights: false
      data_sizes:
        input_bytes: 65536           # (1, 64, 256) * 4B
        output_bytes: 65536
        weight_bytes: 1048576        # 4 * Linear(256,256) = 4 * 256^2 * 4B

    prefill_feedforward:
      codesign_config: vitis_test_sweep_basic_llama_feedforward
      shared_weights: false
      data_sizes:
        input_bytes: 65536
        output_bytes: 65536
        weight_bytes: 3145728        # 3 * 256*1024 * 4B

    # --- Decode phase blocks (one token at a time, uses KV cache) ---
    decode_attention:
      codesign_config: vitis_test_sweep_basic_llama_decode_attention
      shared_weights: false
      data_sizes:
        input_bytes: 132096          # x(1024) + k_cache(65536) + v_cache(65536)
        output_bytes: 1024
        weight_bytes: 1048576        # same weights as prefill attention

    decode_feedforward:
      codesign_config: vitis_test_sweep_basic_llama_decode_feedforward
      shared_weights: false
      data_sizes:
        input_bytes: 1024            # (1, 1, 256) * 4B
        output_bytes: 1024
        weight_bytes: 3145728        # same weights as prefill feedforward

  dfg:
    default_transfer: global_memory

  system_constraints:
    max_total_power: 150.0
    max_total_area: 1.0e12
    objective: edp

# ---------------------------------------------------------------------------
# LLaMA 3 8B (full scale): prefill (64-token prompt) + decode (64 tokens)
# ---------------------------------------------------------------------------
# Real LLaMA 3 8B architecture:
#   32 transformer layers, hidden=4096, FFN SwiGLU intermediate=14336.
#   Grouped-query attention: 32 Q-heads, 8 KV-heads, head_dim=128.
#   Uses Llama38BInference with GQA-accurate sub-blocks (Llama38BPrefillAttention,
#   Llama38BDecodeAttention) — correctly models 32 Q-heads / 8 KV-heads.
#   Weight bytes computed for actual GQA projection sizes.
#   Prompt length is capped at 64 by the hardcoded SEQ_LEN in LlamaPrefillAttention.
#
# Data size accounting:
#   Attention weights (GQA):
#     wq(4096×4096) + wk(4096×1024) + wv(4096×1024) + wo(4096×4096) × 4B
#     [8 KV-heads × head_dim 128 = 1024 KV dim]
#     = (16,777,216 + 4,194,304 + 4,194,304 + 16,777,216) × 4 = 167,772,160 B
#   FFN weights (SwiGLU):
#     w1(4096×14336) + w3(4096×14336) + w2(14336×4096) × 4B
#     = 3 × 4096 × 14336 × 4 = 704,643,072 B
#   Decode KV cache (GQA, 8 KV-heads, cache_len=64):
#     k_cache(1,8,64,128) + v_cache(1,8,64,128) × 4B = 2 × 262,144 = 524,288 B
llama3_8b_system:
  name: "llama3_8b_inference"
  description: "LLaMA 3 8B inference: prefill (64 tokens) + decode (64 tokens)"
  shared_tech: false

  global_memory:
    bandwidth_gbps: 100.0
    latency_ns: 100.0
    energy_per_bit_pj: 10.0
    capacity_bytes: 32.0e9            # ~28 GB model weights at FP32

  benchmark:
    class: Llama38BInference
    module_path: src/benchmarks/vitis/Llama38BInference
    init_args:
      hidden: 4096
      n_heads: 32
      n_kv_heads: 8
      head_dim: 128
      ffn_dim: 14336
      n_layers: 32
      n_decode: 64
      cache_len: 64
    input_shapes:
      - [1, 64, 4096]           # prompt tensor (batch, prompt_len, hidden)
    module_attr_map:
      prefill_attention: prefill_attention
      prefill_feedforward: prefill_feedforward
      decode_attention: decode_attention
      decode_feedforward: decode_feedforward

  block_types:
    # --- Prefill phase blocks (process full prompt) ---
    prefill_attention:
      codesign_config: vitis_test_sweep_llama3_8b_prefill_attention
      shared_weights: false
      data_sizes:
        input_bytes: 1048576             # (1, 64, 4096) × 4B
        output_bytes: 1048576
        weight_bytes: 167772160          # GQA: wq+wk+wv+wo (see above)

    prefill_feedforward:
      codesign_config: vitis_test_sweep_llama3_8b_prefill_feedforward
      shared_weights: false
      data_sizes:
        input_bytes: 1048576             # (1, 64, 4096) × 4B
        output_bytes: 1048576
        weight_bytes: 704643072          # SwiGLU: w1+w3+w2 (see above)

    # --- Decode phase blocks (one token at a time, uses KV cache) ---
    decode_attention:
      codesign_config: vitis_test_sweep_llama3_8b_decode_attention
      shared_weights: false
      data_sizes:
        # x(1,1,4096) + k_cache(1,8,64,128) + v_cache(1,8,64,128)
        input_bytes: 540672              # 16,384 + 262,144 + 262,144
        output_bytes: 16384              # (1, 1, 4096) × 4B
        weight_bytes: 167772160          # same projections as prefill attention

    decode_feedforward:
      codesign_config: vitis_test_sweep_llama3_8b_decode_feedforward
      shared_weights: false
      data_sizes:
        input_bytes: 16384               # (1, 1, 4096) × 4B
        output_bytes: 16384
        weight_bytes: 704643072          # same weights as prefill feedforward

  dfg:
    default_transfer: global_memory

  system_constraints:
    max_total_power: 150.0
    max_total_area: 1.0e12
    objective: edp

# ---------------------------------------------------------------------------
# DeepSeek-V3 (MoE): prefill (512 tokens) + decode (64 tokens)
# ---------------------------------------------------------------------------
# DeepSeek-V3 is a 671B-parameter (37B active) MoE model with 61 layers.
# Layer structure (first_k_dense_replace=3):
#   Layers 1-3:   MLA attention + DENSE SwiGLU FFN (intermediate=18432)
#   Layers 4-61:  MLA attention + SPARSE MoE FFN (256 routed + 1 shared,
#                   top-8 active, moe_intermediate=2048)
# MLA: q_lora_rank=1536, kv_lora_rank=512 for compressed K/V caching.
#
# Data size accounting:
#   MLA attention weights: q_down(7168x1536) + q_up(1536x7168) + kv_down(7168x512)
#     + k_up(512x7168) + v_up(512x7168) + o_proj(7168x7168) all * 4B
#   Dense FFN weights: w1(7168x18432) + w3(7168x18432) + w2(18432x7168) * 4B
#   MoE weights: gate(7168x256) + shared(3x7168x2048) + 8 active routed(3x7168x2048)
deepseekv3_inference_system:
  name: "deepseekv3_inference"
  description: "DeepSeek-V3 MoE inference: prefill (512 tokens) + decode (64 tokens)"
  shared_tech: false

  global_memory:
    bandwidth_gbps: 100.0
    latency_ns: 100.0
    energy_per_bit_pj: 10.0
    capacity_bytes: 64.0e9

  benchmark:
    class: DeepSeekV3Inference
    module_path: src/benchmarks/vitis/DeepSeekV3Inference
    init_args:
      hidden: 7168
      n_heads: 128
      head_dim: 128
      q_lora_rank: 1536
      kv_lora_rank: 512
      dense_intermediate: 18432
      moe_intermediate: 2048
      n_routed_experts: 256
      top_k: 8
      n_layers: 61
      n_dense: 3
      n_decode: 64
      cache_len: 512
    input_shapes:
      - [1, 512, 7168]           # prompt tensor (batch, prompt_len, hidden)
    module_attr_map:
      prefill_attention: prefill_attention
      prefill_dense_ffn: prefill_dense_ffn
      prefill_moe: prefill_moe
      decode_attention: decode_attention
      decode_dense_ffn: decode_dense_ffn
      decode_moe: decode_moe

  block_types:
    # --- Prefill phase ---
    prefill_attention:
      codesign_config: vitis_test_sweep_deepseekv3_prefill_attention
      shared_weights: false
      data_sizes:
        # input/output: (1, 512, 7168) * 4B
        input_bytes: 14680064
        output_bytes: 14680064
        # q_down(7168x1536) + q_up(1536x7168) + kv_down(7168x512)
        # + k_up(512x7168) + v_up(512x7168) + o_proj(7168x7168) all * 4B
        weight_bytes: 455081984

    prefill_dense_ffn:
      codesign_config: vitis_test_sweep_deepseekv3_prefill_dense_ffn
      shared_weights: false
      data_sizes:
        input_bytes: 14680064           # (1, 512, 7168) * 4B
        output_bytes: 14680064
        # w1(7168x18432) + w3(7168x18432) + w2(18432x7168) * 4B
        weight_bytes: 1585446912

    prefill_moe:
      codesign_config: vitis_test_sweep_deepseekv3_prefill_moe
      shared_weights: false
      data_sizes:
        input_bytes: 14680064
        output_bytes: 14680064
        # gate(7168x256) + shared(3x7168x2048) + 8 active routed(3x7168x2048 each)
        # = 7168*256 + 9*3*7168*2048, all * 4B
        weight_bytes: 1592786944

    # --- Decode phase ---
    decode_attention:
      codesign_config: vitis_test_sweep_deepseekv3_decode_attention
      shared_weights: false
      data_sizes:
        # x(1,1,7168) + k_cache(1,128,512,128) + v_cache(1,128,512,128)
        input_bytes: 1077248
        output_bytes: 28672            # (1, 1, 7168) * 4B
        weight_bytes: 455081984        # same weights as prefill attention

    decode_dense_ffn:
      codesign_config: vitis_test_sweep_deepseekv3_decode_dense_ffn
      shared_weights: false
      data_sizes:
        input_bytes: 28672             # (1, 1, 7168) * 4B
        output_bytes: 28672
        weight_bytes: 1585446912       # same weights as prefill dense FFN

    decode_moe:
      codesign_config: vitis_test_sweep_deepseekv3_decode_moe
      shared_weights: false
      data_sizes:
        input_bytes: 28672             # (1, 1, 7168) * 4B
        output_bytes: 28672
        weight_bytes: 1592786944       # same weights as prefill MoE

  dfg:
    default_transfer: global_memory

  system_constraints:
    max_total_power: 150.0
    max_total_area: 1.0e12
    objective: edp

# ---------------------------------------------------------------------------
# Mixtral 8x7B (MoE): prefill (512 tokens) + decode (64 tokens)
# ---------------------------------------------------------------------------
# Mixtral 8x7B is a sparse MoE transformer: 32 layers, grouped-query attention
# (32 Q-heads, 8 KV-heads, head_dim=128), sparse MoE FFN with 8 experts and
# top-2 routing (ffn_intermediate=14336).
#
# Data size accounting:
#   Prefill attention weights: wq(4096x4096) + wk(4096x1024) + wv(4096x1024)
#     + wo(4096x4096) all * 4B  [n_kv_heads=8, head_dim=128 → 1024 KV dim]
#   Prefill MoE weights: gate(4096x8) + top-2 fused SwiGLU(3x4096x28672)
#     [28672 = 14336 * top_k=2]
mixtral_inference_system:
  name: "mixtral_inference"
  description: "Mixtral 8x7B MoE inference: prefill (512 tokens) + decode (64 tokens)"
  shared_tech: false

  global_memory:
    bandwidth_gbps: 100.0
    latency_ns: 100.0
    energy_per_bit_pj: 10.0
    capacity_bytes: 64.0e9

  benchmark:
    class: MixtralInference
    module_path: src/benchmarks/vitis/MixtralInference
    init_args:
      hidden: 4096
      n_heads: 32
      n_kv_heads: 8
      head_dim: 128
      ffn_intermediate: 14336
      n_experts: 8
      top_k: 2
      n_layers: 32
      n_decode: 64
      cache_len: 512
    input_shapes:
      - [1, 512, 4096]           # prompt tensor (batch, prompt_len, hidden)
    module_attr_map:
      prefill_attention: prefill_attention
      prefill_moe: prefill_moe
      decode_attention: decode_attention
      decode_moe: decode_moe

  block_types:
    # --- Prefill phase ---
    prefill_attention:
      codesign_config: vitis_test_sweep_mixtral_prefill_attention
      shared_weights: false
      data_sizes:
        # input/output: (1, 512, 4096) * 4B
        input_bytes: 8388608
        output_bytes: 8388608
        # wq(4096x4096) + wk(4096x1024) + wv(4096x1024) + wo(4096x4096) * 4B
        weight_bytes: 167772160

    prefill_moe:
      codesign_config: vitis_test_sweep_mixtral_prefill_moe
      shared_weights: false
      data_sizes:
        input_bytes: 8388608
        output_bytes: 8388608
        # gate(4096x8) + w1(4096x28672) + w3(4096x28672) + w2(28672x4096) * 4B
        # (28672 = 14336 * top_k=2, fused)
        weight_bytes: 1409417216

    # --- Decode phase ---
    decode_attention:
      codesign_config: vitis_test_sweep_mixtral_decode_attention
      shared_weights: false
      data_sizes:
        # x(1,1,4096) + k_cache(1,8,512,128) + v_cache(1,8,512,128)
        input_bytes: 4210688
        output_bytes: 16384            # (1, 1, 4096) * 4B
        weight_bytes: 167772160        # same weights as prefill attention

    decode_moe:
      codesign_config: vitis_test_sweep_mixtral_decode_moe
      shared_weights: false
      data_sizes:
        input_bytes: 16384             # (1, 1, 4096) * 4B
        output_bytes: 16384
        weight_bytes: 1409417216       # same weights as prefill MoE

  dfg:
    default_transfer: global_memory

  system_constraints:
    max_total_power: 150.0
    max_total_area: 1.0e12
    objective: edp

# ---------------------------------------------------------------------------
# V-JEPA: context encoder (ViT-H/16) + predictor transformer
# ---------------------------------------------------------------------------
# V-JEPA has two networks:
#   Context encoder (ViT-H/16, 32 layers): factorised space-time attention.
#     Each block: spatial attention (196 patches/frame, per-frame) →
#                 temporal attention (all 3136 tokens) → FFN.
#   Predictor (12 layers, hidden=384): takes encoder outputs projected to
#     pred_hidden=384 plus mask tokens; predicts target patch representations.
#     Input: 3136 positions (all video patches).
#
# Data size accounting:
#   Spatial attention: called with (n_frames * batch, 196, 1280) = (16, 196, 1280)
#     input_bytes = 16 * 196 * 1280 * 4B = 16,056,320
#   Temporal attention: (1, 3136, 1280) * 4B = 16,056,320
#   FFN: same 16,056,320
#   Predictor attention: (1, 3136, 384) * 4B = 4,825,088
#   Predictor FFN: same 4,825,088
vjepa_inference_system:
  name: "vjepa_inference"
  description: "V-JEPA ViT-H/16 context encoder + predictor: 16 frames × 196 patches"
  shared_tech: false

  global_memory:
    bandwidth_gbps: 100.0
    latency_ns: 100.0
    energy_per_bit_pj: 10.0
    capacity_bytes: 16.0e9

  benchmark:
    class: VJEPAInference
    module_path: src/benchmarks/vitis/VJEPAInference
    init_args:
      hidden: 1280
      n_heads: 16
      head_dim: 80
      ffn_intermediate: 5120
      n_frames: 16
      spatial_patches: 196
      n_layers: 32
      pred_hidden: 384
      pred_n_heads: 12
      pred_head_dim: 32
      pred_ffn: 1536
      n_pred_layers: 12
    input_shapes:
      - [1, 3136, 1280]          # all video tokens (batch, total_patches, hidden)
    module_attr_map:
      spatial_attention: spatial_attention
      temporal_attention: temporal_attention
      feedforward: feedforward
      predictor_attention: predictor_attention
      predictor_feedforward: predictor_feedforward

  block_types:
    # --- Context encoder ---
    spatial_attention:
      codesign_config: vitis_test_sweep_vjepa_spatial_attention
      shared_weights: false
      data_sizes:
        # input shape after view: (batch * n_frames, spatial_patches, hidden)
        #                       = (16, 196, 1280) → 16 * 196 * 1280 * 4B
        input_bytes: 16056320
        output_bytes: 16056320
        # 4 × Linear(1280, 1280) * 4B
        weight_bytes: 26214400

    temporal_attention:
      codesign_config: vitis_test_sweep_vjepa_temporal_attention
      shared_weights: false
      data_sizes:
        # input/output: (1, 3136, 1280) * 4B
        input_bytes: 16056320
        output_bytes: 16056320
        weight_bytes: 26214400

    feedforward:
      codesign_config: vitis_test_sweep_vjepa_feedforward
      shared_weights: false
      data_sizes:
        input_bytes: 16056320
        output_bytes: 16056320
        # fc1(1280x5120) + fc2(5120x1280) * 4B
        weight_bytes: 52428800

    # --- Predictor ---
    predictor_attention:
      codesign_config: vitis_test_sweep_vjepa_predictor_attention
      shared_weights: false
      data_sizes:
        # input/output: (1, 3136, 384) * 4B
        input_bytes: 4825088
        output_bytes: 4825088
        # 4 × Linear(384, 384) * 4B
        weight_bytes: 2359296

    predictor_feedforward:
      codesign_config: vitis_test_sweep_vjepa_predictor_feedforward
      shared_weights: false
      data_sizes:
        input_bytes: 4825088
        output_bytes: 4825088
        # fc1(384x1536) + fc2(1536x384) * 4B
        weight_bytes: 4718592

  dfg:
    default_transfer: global_memory

  system_constraints:
    max_total_power: 150.0
    max_total_area: 1.0e12
    objective: edp

# ---------------------------------------------------------------------------
# OpenVLA: dual vision encoding + projection + LM prefill + action decode
# ---------------------------------------------------------------------------
# OpenVLA uses Prismatic dual-encoder architecture (7B parameters total):
#   DINOv2 ViT-L/14 (24 layers, hidden=1024): semantic/structural features
#   SigLIP-SO400M   (27 layers, hidden=1152): vision-language aligned features
#   Both process the same 256 patches (224×224 @ 14px patch size).
#   Vision projection MLP: concat(DINOv2=1024, SigLIP=1152)=2176 → lm_hidden=4096
#   LM: Llama 2 7B (32 layers, hidden=4096) prefill + 7 action token decode.
#
# Data size accounting:
#   DINOv2 attention: 4 × Linear(1024, 1024) * 4B = 16,777,216B
#   DINOv2 FFN: fc1(1024x4096) + fc2(4096x1024) * 4B = 33,554,432B
#   SigLIP attention: 4 × Linear(1152, 1152) * 4B = 21,233,664B
#   SigLIP FFN: fc1(1152x4304) + fc2(4304x1152) * 4B = 39,583,744B
#   Vision projection: fc1(2176x4096) + fc2(4096x4096) * 4B = 102,760,448B
#   LM attention: 4 × Linear(4096, 4096) * 4B = 268,435,456B
#   LM FFN: w1(4096x11008) + w3(4096x11008) + w2(11008x4096) * 4B = 541,065,216B
openvla_inference_system:
  name: "openvla_inference"
  description: "OpenVLA inference: DINOv2 + SigLIP dual-encoder + Llama2-7B LM + 7-token action decode"
  shared_tech: false

  global_memory:
    bandwidth_gbps: 100.0
    latency_ns: 100.0
    energy_per_bit_pj: 10.0
    capacity_bytes: 32.0e9

  benchmark:
    class: OpenVLAInference
    module_path: src/benchmarks/vitis/OpenVLAInference
    init_args:
      # DINOv2 ViT-L/14
      dino_hidden: 1024
      dino_n_heads: 16
      dino_head_dim: 64
      dino_ffn: 4096
      n_dino_layers: 24
      # SigLIP-SO400M
      vis_hidden: 1152
      vis_n_heads: 16
      vis_head_dim: 72
      vis_ffn: 4304
      n_vision_layers: 27
      n_patches: 256
      # Language model (Llama 2 7B)
      lm_hidden: 4096
      lm_n_heads: 32
      lm_head_dim: 128
      lm_ffn: 11008
      n_lm_layers: 32
      # Decode
      n_decode: 7
      cache_len: 512
    input_shapes:
      - [1, 256, 1024]           # DINOv2 patch embeddings (batch, n_patches, dino_hidden)
      - [1, 256, 1152]           # SigLIP patch embeddings (batch, n_patches, vis_hidden)
      - [1, 256, 4096]           # language tokens        (batch, lang_len, lm_hidden)
    module_attr_map:
      dino_attention: dino_attention
      dino_feedforward: dino_feedforward
      vision_attention: vision_attention
      vision_feedforward: vision_feedforward
      vision_projection: vision_projection
      lm_prefill_attention: lm_prefill_attention
      lm_feedforward: lm_feedforward
      lm_decode_attention: lm_decode_attention

  block_types:
    # --- DINOv2 encoder (ViT-L/14, 24 layers) ---
    dino_attention:
      codesign_config: vitis_test_sweep_openvla_dino_attention
      shared_weights: false
      data_sizes:
        # input/output: (1, 256, 1024) * 4B
        input_bytes: 1048576
        output_bytes: 1048576
        # 4 × Linear(1024, 1024) * 4B  [n_heads*head_dim = 16*64 = 1024]
        weight_bytes: 16777216

    dino_feedforward:
      codesign_config: vitis_test_sweep_openvla_dino_feedforward
      shared_weights: false
      data_sizes:
        input_bytes: 1048576
        output_bytes: 1048576
        # fc1(1024x4096) + fc2(4096x1024) * 4B
        weight_bytes: 33554432

    # --- SigLIP encoder (SO400M, 27 layers) ---
    vision_attention:
      codesign_config: vitis_test_sweep_openvla_vision_attention
      shared_weights: false
      data_sizes:
        # input/output: (1, 256, 1152) * 4B
        input_bytes: 1179648
        output_bytes: 1179648
        # 4 × Linear(1152, 1152) * 4B  [n_heads*head_dim = 16*72 = 1152]
        weight_bytes: 21233664

    vision_feedforward:
      codesign_config: vitis_test_sweep_openvla_vision_feedforward
      shared_weights: false
      data_sizes:
        input_bytes: 1179648
        output_bytes: 1179648
        # fc1(1152x4304) + fc2(4304x1152) * 4B
        weight_bytes: 39583744

    # --- Vision projection MLP (concat 2176 → lm_hidden 4096) ---
    vision_projection:
      codesign_config: vitis_test_sweep_openvla_vision_projection
      shared_weights: false
      data_sizes:
        # input: (1, 256, 2176) * 4B  [concat DINOv2=1024 + SigLIP=1152]
        input_bytes: 2228224
        # output: (1, 256, 4096) * 4B
        output_bytes: 4194304
        # fc1(2176x4096) + fc2(4096x4096) * 4B
        weight_bytes: 102760448

    # --- Language model prefill (Llama 2 7B) ---
    lm_prefill_attention:
      codesign_config: vitis_test_sweep_openvla_lm_prefill_attention
      shared_weights: false
      data_sizes:
        # input/output: (1, 512, 4096) * 4B  [256 vision + 256 language tokens]
        input_bytes: 8388608
        output_bytes: 8388608
        # 4 × Linear(4096, 4096) * 4B
        weight_bytes: 268435456

    lm_feedforward:
      codesign_config: vitis_test_sweep_openvla_lm_feedforward
      shared_weights: false
      data_sizes:
        input_bytes: 8388608
        output_bytes: 8388608
        # w1(4096x11008) + w3(4096x11008) + w2(11008x4096) * 4B
        weight_bytes: 541065216

    # --- Action decode ---
    lm_decode_attention:
      codesign_config: vitis_test_sweep_openvla_lm_decode_attention
      shared_weights: false
      data_sizes:
        # x(1,1,4096) + k_cache(1,32,512,128) + v_cache(1,32,512,128)
        input_bytes: 16793600
        output_bytes: 16384            # (1, 1, 4096) * 4B
        weight_bytes: 268435456        # same weights as prefill attention

  dfg:
    default_transfer: global_memory

  system_constraints:
    max_total_power: 150.0
    max_total_area: 1.0e12
    objective: edp

# ---------------------------------------------------------------------------
# DeepSeek-V3 (MoE) SMALL: scaled-down for rapid prototyping
# ---------------------------------------------------------------------------
# Reduced: hidden=512, n_heads=8, head_dim=64, q_lora_rank=128,
#   kv_lora_rank=64, dense_intermediate=1024, moe_intermediate=256,
#   n_routed_experts=16, top_k=2, n_layers=8, prompt_len=64, cache_len=64.
deepseekv3_small_system:
  name: "deepseekv3_small"
  description: "DeepSeek-V3 small (hidden=512): prefill (64 tokens) + decode (16 tokens)"
  shared_tech: false

  global_memory:
    bandwidth_gbps: 100.0
    latency_ns: 100.0
    energy_per_bit_pj: 10.0
    capacity_bytes: 16.0e9

  benchmark:
    class: DeepSeekV3Inference
    module_path: src/benchmarks/vitis/DeepSeekV3Inference
    init_args:
      hidden: 512
      n_heads: 8
      head_dim: 64
      q_lora_rank: 128
      kv_lora_rank: 64
      dense_intermediate: 1024
      moe_intermediate: 256
      n_routed_experts: 16
      top_k: 2
      n_layers: 8
      n_dense: 2
      n_decode: 16
      cache_len: 64
    input_shapes:
      - [1, 64, 512]             # prompt tensor (batch, prompt_len, hidden)
    module_attr_map:
      prefill_attention: prefill_attention
      prefill_dense_ffn: prefill_dense_ffn
      prefill_moe: prefill_moe
      decode_attention: decode_attention
      decode_dense_ffn: decode_dense_ffn
      decode_moe: decode_moe

  block_types:
    # --- Prefill phase ---
    prefill_attention:
      codesign_config: vitis_test_sweep_deepseekv3_prefill_attention_small
      shared_weights: false
      data_sizes:
        # input/output: (1, 64, 512) * 4B
        input_bytes: 131072
        output_bytes: 131072
        # q_down(512x128) + q_up(128x512) + kv_down(512x64)
        # + k_up(64x512) + v_up(64x512) + o_proj(512x512) all * 4B
        weight_bytes: 1966080

    prefill_dense_ffn:
      codesign_config: vitis_test_sweep_deepseekv3_prefill_dense_ffn_small
      shared_weights: false
      data_sizes:
        input_bytes: 131072              # (1, 64, 512) * 4B
        output_bytes: 131072
        # w1(512x1024) + w3(512x1024) + w2(1024x512) * 4B
        weight_bytes: 6291456

    prefill_moe:
      codesign_config: vitis_test_sweep_deepseekv3_prefill_moe_small
      shared_weights: false
      data_sizes:
        input_bytes: 131072
        output_bytes: 131072
        # gate(512x16) + shared(3x512x256) + 2 active routed(3x512x256 each)
        # = 512*16 + 9*512*256, all * 4B
        weight_bytes: 4751360

    # --- Decode phase ---
    decode_attention:
      codesign_config: vitis_test_sweep_deepseekv3_decode_attention_small
      shared_weights: false
      data_sizes:
        # x(1,1,512) + k_cache(1,8,64,64) + v_cache(1,8,64,64)
        input_bytes: 264192
        output_bytes: 2048               # (1, 1, 512) * 4B
        weight_bytes: 1966080            # same weights as prefill attention

    decode_dense_ffn:
      codesign_config: vitis_test_sweep_deepseekv3_decode_dense_ffn_small
      shared_weights: false
      data_sizes:
        input_bytes: 2048                # (1, 1, 512) * 4B
        output_bytes: 2048
        weight_bytes: 6291456            # same weights as prefill dense FFN

    decode_moe:
      codesign_config: vitis_test_sweep_deepseekv3_decode_moe_small
      shared_weights: false
      data_sizes:
        input_bytes: 2048                # (1, 1, 512) * 4B
        output_bytes: 2048
        weight_bytes: 4751360            # same weights as prefill MoE

  dfg:
    default_transfer: global_memory

  system_constraints:
    max_total_power: 150.0
    max_total_area: 1.0e12
    objective: edp

# ---------------------------------------------------------------------------
# Mixtral 8x7B (MoE) SMALL: scaled-down for rapid prototyping
# ---------------------------------------------------------------------------
# Reduced: hidden=512, n_heads=8, n_kv_heads=2, head_dim=64,
#   ffn_intermediate=1024, n_experts=4, top_k=2, n_layers=8,
#   prompt_len=64, cache_len=64.
mixtral_small_system:
  name: "mixtral_small"
  description: "Mixtral small (hidden=512): prefill (64 tokens) + decode (16 tokens)"
  shared_tech: false

  global_memory:
    bandwidth_gbps: 100.0
    latency_ns: 100.0
    energy_per_bit_pj: 10.0
    capacity_bytes: 16.0e9

  benchmark:
    class: MixtralInference
    module_path: src/benchmarks/vitis/MixtralInference
    init_args:
      hidden: 512
      n_heads: 8
      n_kv_heads: 2
      head_dim: 64
      ffn_intermediate: 1024
      n_experts: 4
      top_k: 2
      n_layers: 8
      n_decode: 16
      cache_len: 64
    input_shapes:
      - [1, 64, 512]             # prompt tensor (batch, prompt_len, hidden)
    module_attr_map:
      prefill_attention: prefill_attention
      prefill_moe: prefill_moe
      decode_attention: decode_attention
      decode_moe: decode_moe

  block_types:
    # --- Prefill phase ---
    prefill_attention:
      codesign_config: vitis_test_sweep_mixtral_prefill_attention_small
      shared_weights: false
      data_sizes:
        # input/output: (1, 64, 512) * 4B
        input_bytes: 131072
        output_bytes: 131072
        # wq(512x512) + wk(512x128) + wv(512x128) + wo(512x512) * 4B
        # [n_kv_heads=2, head_dim=64 → 128 KV dim]
        weight_bytes: 2621440

    prefill_moe:
      codesign_config: vitis_test_sweep_mixtral_prefill_moe_small
      shared_weights: false
      data_sizes:
        input_bytes: 131072
        output_bytes: 131072
        # gate(512x4) + w1(512x2048) + w3(512x2048) + w2(2048x512) * 4B
        # (2048 = 1024 * top_k=2, fused)
        weight_bytes: 12591104

    # --- Decode phase ---
    decode_attention:
      codesign_config: vitis_test_sweep_mixtral_decode_attention_small
      shared_weights: false
      data_sizes:
        # x(1,1,512) + k_cache(1,2,64,64) + v_cache(1,2,64,64)
        input_bytes: 67584
        output_bytes: 2048               # (1, 1, 512) * 4B
        weight_bytes: 2621440            # same weights as prefill attention

    decode_moe:
      codesign_config: vitis_test_sweep_mixtral_decode_moe_small
      shared_weights: false
      data_sizes:
        input_bytes: 2048                # (1, 1, 512) * 4B
        output_bytes: 2048
        weight_bytes: 12591104           # same weights as prefill MoE

  dfg:
    default_transfer: global_memory

  system_constraints:
    max_total_power: 150.0
    max_total_area: 1.0e12
    objective: edp

# ---------------------------------------------------------------------------
# V-JEPA SMALL: scaled-down for rapid prototyping
# ---------------------------------------------------------------------------
# Reduced: hidden=256, n_heads=4, head_dim=64, ffn_intermediate=1024,
#   n_frames=4 (→ 784 total patches), n_layers=8,
#   pred_hidden=128, pred_ffn=512, n_pred_layers=4.
vjepa_small_system:
  name: "vjepa_small"
  description: "V-JEPA small (hidden=256): 4 frames × 196 patches, 8-layer encoder"
  shared_tech: false

  global_memory:
    bandwidth_gbps: 100.0
    latency_ns: 100.0
    energy_per_bit_pj: 10.0
    capacity_bytes: 4.0e9

  benchmark:
    class: VJEPAInference
    module_path: src/benchmarks/vitis/VJEPAInference
    init_args:
      hidden: 256
      n_heads: 4
      head_dim: 64
      ffn_intermediate: 1024
      n_frames: 4
      spatial_patches: 196
      n_layers: 8
      pred_hidden: 128
      pred_n_heads: 4
      pred_head_dim: 32
      pred_ffn: 512
      n_pred_layers: 4
    input_shapes:
      - [1, 784, 256]            # all video tokens (batch, n_frames*spatial_patches, hidden)
    module_attr_map:
      spatial_attention: spatial_attention
      temporal_attention: temporal_attention
      feedforward: feedforward
      predictor_attention: predictor_attention
      predictor_feedforward: predictor_feedforward

  block_types:
    # --- Context encoder ---
    spatial_attention:
      codesign_config: vitis_test_sweep_vjepa_spatial_attention_small
      shared_weights: false
      data_sizes:
        # input shape after view: (batch * n_frames, spatial_patches, hidden)
        #                       = (4, 196, 256) → 4 * 196 * 256 * 4B
        input_bytes: 802816
        output_bytes: 802816
        # 4 × Linear(256, 256) * 4B
        weight_bytes: 1048576

    temporal_attention:
      codesign_config: vitis_test_sweep_vjepa_temporal_attention_small
      shared_weights: false
      data_sizes:
        # input/output: (1, 784, 256) * 4B
        input_bytes: 802816
        output_bytes: 802816
        weight_bytes: 1048576

    feedforward:
      codesign_config: vitis_test_sweep_vjepa_feedforward_small
      shared_weights: false
      data_sizes:
        input_bytes: 802816
        output_bytes: 802816
        # fc1(256x1024) + fc2(1024x256) * 4B
        weight_bytes: 2097152

    # --- Predictor ---
    predictor_attention:
      codesign_config: vitis_test_sweep_vjepa_predictor_attention_small
      shared_weights: false
      data_sizes:
        # input/output: (1, 784, 128) * 4B
        input_bytes: 401408
        output_bytes: 401408
        # 4 × Linear(128, 128) * 4B
        weight_bytes: 262144

    predictor_feedforward:
      codesign_config: vitis_test_sweep_vjepa_predictor_feedforward_small
      shared_weights: false
      data_sizes:
        input_bytes: 401408
        output_bytes: 401408
        # fc1(128x512) + fc2(512x128) * 4B
        weight_bytes: 524288

  dfg:
    default_transfer: global_memory

  system_constraints:
    max_total_power: 150.0
    max_total_area: 1.0e12
    objective: edp

# ---------------------------------------------------------------------------
# ResNet-18: image classification (224x224 → 1000 classes)
# ---------------------------------------------------------------------------
# Standard ResNet-18 architecture (11.7M parameters).  Each BasicBlock is
# exposed as its own codesign block type — stacking two blocks in one module
# makes the StreamHLS permutation space too large.
#
# Block dimensions (batch=1):
#   stem:         (3,224,224)  → (64,56,56)   via conv7x7 + 2×maxpool
#   layer1_block: (64,56,56)   → (64,56,56)   BasicBlock(64→64)    called 2×
#   layer2_down:  (64,56,56)   → (128,28,28)  BasicBlock(64→128,s=2)
#   layer2_block: (128,28,28)  → (128,28,28)  BasicBlock(128→128)
#   layer3_down:  (128,28,28)  → (256,14,14)  BasicBlock(128→256,s=2)
#   layer3_block: (256,14,14)  → (256,14,14)  BasicBlock(256→256)
#   layer4_down:  (256,14,14)  → (512,7,7)    BasicBlock(256→512,s=2)
#   layer4_block: (512,7,7)    → (512,7,7)    BasicBlock(512→512)
#   classifier:   (512,7,7)    → (1000,)      AdaptiveAvgPool + fc
#
# Weight accounting per block (conv + BN weights; BN = weight+bias per channel):
#   stem:         conv1(64×3×7×7) * 4B                            =    37,632 bytes
#   layer1_block: (64×64×9 + 64×2 + 64×64×9 + 64×2) * 4B        =   590,848 bytes
#   layer2_down:  (128×64×9 + 128×2 + 128×128×9 + 128×2
#                  + 128×64 + 128×2) * 4B                         =   920,576 bytes
#   layer2_block: (128×128×9 + 128×2 + 128×128×9 + 128×2) * 4B  = 1,181,696 bytes
#   layer3_down:  (256×128×9 + 256×2 + 256×256×9 + 256×2
#                  + 256×128 + 256×2) * 4B                        = 3,676,160 bytes
#   layer3_block: (256×256×9 + 256×2 + 256×256×9 + 256×2) * 4B  = 4,722,688 bytes
#   layer4_down:  (512×256×9 + 512×2 + 512×512×9 + 512×2
#                  + 512×256 + 512×2) * 4B                        = 14,692,352 bytes
#   layer4_block: (512×512×9 + 512×2 + 512×512×9 + 512×2) * 4B  = 18,882,560 bytes
#   classifier:   (512×1000 + 1000) * 4B                         =  2,052,000 bytes
resnet18_system:
  name: "resnet18_inference"
  description: "ResNet-18 image classification: 224x224 input, 1000-class output"
  shared_tech: false

  global_memory:
    bandwidth_gbps: 100.0
    latency_ns: 100.0
    energy_per_bit_pj: 10.0
    capacity_bytes: 1.0e9

  benchmark:
    class: ResNet18Inference
    module_path: src/benchmarks/vitis/ResNet18Inference
    init_args:
      num_classes: 1000
    input_shapes:
      - [1, 3, 224, 224]           # image tensor (batch, channels, H, W)
    module_attr_map:
      stem: stem
      layer1_block: layer1_block
      layer2_down: layer2_down
      layer2_block: layer2_block
      layer3_down: layer3_down
      layer3_block: layer3_block
      layer4_down: layer4_down
      layer4_block: layer4_block
      classifier: classifier

  block_types:
    # Initial conv7x7 + 2x maxpool
    stem:
      codesign_config: vitis_test_sweep_resnet18_stem
      shared_weights: false
      data_sizes:
        input_bytes: 602112          # (1, 3, 224, 224) * 4B
        output_bytes: 802816         # (1, 64, 56, 56) * 4B
        weight_bytes: 37632          # conv1(64×3×7×7) * 4B

    # BasicBlock(64→64, stride=1) — called 2× (both blocks of layer1 identical)
    layer1_block:
      codesign_config: vitis_test_sweep_resnet18_layer1_block
      shared_weights: false
      data_sizes:
        input_bytes: 802816          # (1, 64, 56, 56) * 4B
        output_bytes: 802816
        weight_bytes: 590848         # (64×64×9+64×2+64×64×9+64×2) * 4B

    # BasicBlock(64→128, stride=2) — first block of layer2, with downsample
    layer2_down:
      codesign_config: vitis_test_sweep_resnet18_layer2_down
      shared_weights: false
      data_sizes:
        input_bytes: 802816          # (1, 64, 56, 56) * 4B
        output_bytes: 401408         # (1, 128, 28, 28) * 4B
        weight_bytes: 920576         # (128×64×9+128×2+128×128×9+128×2+128×64+128×2) * 4B

    # BasicBlock(128→128, stride=1) — second block of layer2
    layer2_block:
      codesign_config: vitis_test_sweep_resnet18_layer2_block
      shared_weights: false
      data_sizes:
        input_bytes: 401408          # (1, 128, 28, 28) * 4B
        output_bytes: 401408
        weight_bytes: 1181696        # (128×128×9+128×2+128×128×9+128×2) * 4B

    # BasicBlock(128→256, stride=2) — first block of layer3, with downsample
    layer3_down:
      codesign_config: vitis_test_sweep_resnet18_layer3_down
      shared_weights: false
      data_sizes:
        input_bytes: 401408          # (1, 128, 28, 28) * 4B
        output_bytes: 200704         # (1, 256, 14, 14) * 4B
        weight_bytes: 3676160        # (256×128×9+256×2+256×256×9+256×2+256×128+256×2) * 4B

    # BasicBlock(256→256, stride=1) — second block of layer3
    layer3_block:
      codesign_config: vitis_test_sweep_resnet18_layer3_block
      shared_weights: false
      data_sizes:
        input_bytes: 200704          # (1, 256, 14, 14) * 4B
        output_bytes: 200704
        weight_bytes: 4722688        # (256×256×9+256×2+256×256×9+256×2) * 4B

    # BasicBlock(256→512, stride=2) — first block of layer4, with downsample
    layer4_down:
      codesign_config: vitis_test_sweep_resnet18_layer4_down
      shared_weights: false
      data_sizes:
        input_bytes: 200704          # (1, 256, 14, 14) * 4B
        output_bytes: 100352         # (1, 512, 7, 7) * 4B
        weight_bytes: 14692352       # (512×256×9+512×2+512×512×9+512×2+512×256+512×2) * 4B

    # BasicBlock(512→512, stride=1) — second block of layer4
    layer4_block:
      codesign_config: vitis_test_sweep_resnet18_layer4_block
      shared_weights: false
      data_sizes:
        input_bytes: 100352          # (1, 512, 7, 7) * 4B
        output_bytes: 100352
        weight_bytes: 18882560       # (512×512×9+512×2+512×512×9+512×2) * 4B

    # AdaptiveAvgPool2d + flatten + Linear(512→1000)
    classifier:
      codesign_config: vitis_test_sweep_resnet18_classifier
      shared_weights: false
      data_sizes:
        input_bytes: 100352          # (1, 512, 7, 7) * 4B
        output_bytes: 4000           # (1, 1000) * 4B
        weight_bytes: 2052000        # (512×1000 + 1000) * 4B

  dfg:
    default_transfer: global_memory

  system_constraints:
    max_total_power: 150.0
    max_total_area: 1.0e12
    objective: edp

# ---------------------------------------------------------------------------
# OpenVLA SMALL: scaled-down for rapid prototyping
# ---------------------------------------------------------------------------
# Reduced: dino_hidden=256, vis_hidden=256, lm_hidden=512, n_patches=64,
#   dino/vis n_heads=4, head_dim=64, ffn=1024; lm n_heads=8, head_dim=64,
#   lm_ffn=2048; n_dino_layers=6, n_vision_layers=6, n_lm_layers=8,
#   cache_len=128.
openvla_small_system:
  name: "openvla_small"
  description: "OpenVLA small: DINOv2(256)+SigLIP(256)+LM(512), 64 patches, 7-token decode"
  shared_tech: false

  global_memory:
    bandwidth_gbps: 100.0
    latency_ns: 100.0
    energy_per_bit_pj: 10.0
    capacity_bytes: 8.0e9

  benchmark:
    class: OpenVLAInference
    module_path: src/benchmarks/vitis/OpenVLAInference
    init_args:
      # DINOv2 ViT-L/14 (scaled)
      dino_hidden: 256
      dino_n_heads: 4
      dino_head_dim: 64
      dino_ffn: 1024
      n_dino_layers: 6
      # SigLIP (scaled)
      vis_hidden: 256
      vis_n_heads: 4
      vis_head_dim: 64
      vis_ffn: 1024
      n_vision_layers: 6
      n_patches: 64
      # Language model (scaled)
      lm_hidden: 512
      lm_n_heads: 8
      lm_head_dim: 64
      lm_ffn: 2048
      n_lm_layers: 8
      # Decode
      n_decode: 7
      cache_len: 128
    input_shapes:
      - [1, 64, 256]             # DINOv2 patch embeddings (batch, n_patches, dino_hidden)
      - [1, 64, 256]             # SigLIP patch embeddings (batch, n_patches, vis_hidden)
      - [1, 64, 512]             # language tokens        (batch, lang_len, lm_hidden)
    module_attr_map:
      dino_attention: dino_attention
      dino_feedforward: dino_feedforward
      vision_attention: vision_attention
      vision_feedforward: vision_feedforward
      vision_projection: vision_projection
      lm_prefill_attention: lm_prefill_attention
      lm_feedforward: lm_feedforward
      lm_decode_attention: lm_decode_attention

  block_types:
    # --- DINOv2 encoder (scaled, 6 layers) ---
    dino_attention:
      codesign_config: vitis_test_sweep_openvla_dino_attention_small
      shared_weights: false
      data_sizes:
        # input/output: (1, 64, 256) * 4B
        input_bytes: 65536
        output_bytes: 65536
        # 4 × Linear(256, 256) * 4B  [n_heads*head_dim = 4*64 = 256]
        weight_bytes: 1048576

    dino_feedforward:
      codesign_config: vitis_test_sweep_openvla_dino_feedforward_small
      shared_weights: false
      data_sizes:
        input_bytes: 65536
        output_bytes: 65536
        # fc1(256x1024) + fc2(1024x256) * 4B
        weight_bytes: 2097152

    # --- SigLIP encoder (scaled, 6 layers) ---
    vision_attention:
      codesign_config: vitis_test_sweep_openvla_vision_attention_small
      shared_weights: false
      data_sizes:
        # input/output: (1, 64, 256) * 4B
        input_bytes: 65536
        output_bytes: 65536
        # 4 × Linear(256, 256) * 4B  [n_heads*head_dim = 4*64 = 256]
        weight_bytes: 1048576

    vision_feedforward:
      codesign_config: vitis_test_sweep_openvla_vision_feedforward_small
      shared_weights: false
      data_sizes:
        input_bytes: 65536
        output_bytes: 65536
        # fc1(256x1024) + fc2(1024x256) * 4B
        weight_bytes: 2097152

    # --- Vision projection MLP (concat 512 → lm_hidden 512) ---
    vision_projection:
      codesign_config: vitis_test_sweep_openvla_vision_projection_small
      shared_weights: false
      data_sizes:
        # input: (1, 64, 512) * 4B  [concat DINOv2=256 + SigLIP=256]
        input_bytes: 131072
        # output: (1, 64, 512) * 4B
        output_bytes: 131072
        # fc1(512x512) + fc2(512x512) * 4B
        weight_bytes: 2097152

    # --- Language model prefill (scaled, 8 layers) ---
    lm_prefill_attention:
      codesign_config: vitis_test_sweep_openvla_lm_prefill_attention_small
      shared_weights: false
      data_sizes:
        # input/output: (1, 128, 512) * 4B  [64 vision + 64 language tokens]
        input_bytes: 262144
        output_bytes: 262144
        # 4 × Linear(512, 512) * 4B
        weight_bytes: 4194304

    lm_feedforward:
      codesign_config: vitis_test_sweep_openvla_lm_feedforward_small
      shared_weights: false
      data_sizes:
        input_bytes: 262144
        output_bytes: 262144
        # w1(512x2048) + w3(512x2048) + w2(2048x512) * 4B
        weight_bytes: 12582912

    # --- Action decode ---
    lm_decode_attention:
      codesign_config: vitis_test_sweep_openvla_lm_decode_attention_small
      shared_weights: false
      data_sizes:
        # x(1,1,512) + k_cache(1,8,128,64) + v_cache(1,8,128,64)
        input_bytes: 526336
        output_bytes: 2048               # (1, 1, 512) * 4B
        weight_bytes: 4194304            # same weights as prefill attention

  dfg:
    default_transfer: global_memory

  system_constraints:
    max_total_power: 150.0
    max_total_area: 1.0e12
    objective: edp
