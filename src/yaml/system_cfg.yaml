# LLaMA 3 8B full inference: prefill (prompt_len=64) + decode (64 tokens).
# Uses reduced dimensions (dim=256, ffn_dim=1024, n_heads=4).
# Prefill processes the prompt in one shot; decode generates tokens
# autoregressively, each decode step attending to the KV cache.
#
# The DFG is extracted automatically by tracing the LlamaInference
# PyTorch module's forward pass. Sub-module calls are recorded and
# pattern-matched to block_types via module_attr_map.
llama3_inference_system:
  name: "llama3_inference"
  description: "LLaMA 3 8B inference: prefill (64 tokens) + decode (64 tokens)"
  shared_tech: false

  global_memory:
    bandwidth_gbps: 100.0
    latency_ns: 100.0
    energy_per_bit_pj: 10.0
    capacity_bytes: 16.0e9

  # Benchmark module defines the inference workflow (prefill + decode loops).
  # system_codesign traces forward() to build the DFG automatically.
  benchmark:
    class: LlamaInference
    module_path: src/benchmarks/vitis/LlamaInference
    init_args:
      dim: 256
      n_heads: 4
      ffn_dim: 1024
      n_layers: 32
      n_decode: 64
      cache_len: 64
    input_shapes:
      - [1, 64, 256]           # prompt tensor (batch, prompt_len, dim)
    module_attr_map:
      prefill_attention: prefill_attention
      prefill_feedforward: prefill_feedforward
      decode_attention: decode_attention
      decode_feedforward: decode_feedforward

  block_types:
    # --- Prefill phase blocks (process full prompt) ---
    prefill_attention:
      codesign_config: vitis_test_sweep_basic_llama_prefill_attention
      shared_weights: false
      data_sizes:
        input_bytes: 65536           # (1, 64, 256) * 4B
        output_bytes: 65536
        weight_bytes: 1048576        # 4 * Linear(256,256) = 4 * 256^2 * 4B

    prefill_feedforward:
      codesign_config: vitis_test_sweep_basic_llama_feedforward
      shared_weights: false
      data_sizes:
        input_bytes: 65536
        output_bytes: 65536
        weight_bytes: 3145728        # 3 * 256*1024 * 4B

    # --- Decode phase blocks (one token at a time, uses KV cache) ---
    decode_attention:
      codesign_config: vitis_test_sweep_basic_llama_decode_attention
      shared_weights: false
      data_sizes:
        input_bytes: 132096          # x(1024) + k_cache(65536) + v_cache(65536)
        output_bytes: 1024
        weight_bytes: 1048576        # same weights as prefill attention

    decode_feedforward:
      codesign_config: vitis_test_sweep_basic_llama_decode_feedforward
      shared_weights: false
      data_sizes:
        input_bytes: 1024            # (1, 1, 256) * 4B
        output_bytes: 1024
        weight_bytes: 3145728        # same weights as prefill feedforward

  dfg:
    default_transfer: global_memory

  system_constraints:
    max_total_power: 150.0
    max_total_area: 1.0e12
    objective: edp
